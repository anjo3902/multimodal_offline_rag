# Multimodal RAG Configuration
# Copy this file to .env and update with your settings

# ========================================
# LLM Model Configuration
# ========================================
# Ollama model name (phi3, mistral, llama2, etc.)
LLAMA_MODEL_PATH=mistral

# ========================================
# GPU Acceleration (Optional)
# ========================================
# CUDA device (cuda, cpu)
EMBED_DEVICE=cuda

# Windows cuDNN path (if using GPU)
# CUDNN_PATH=C:\Program Files\NVIDIA\CUDNN\v9.5\bin\12.6

# Linux cuDNN path
# CUDNN_PATH=/usr/local/cuda/lib64

# ========================================
# Whisper Audio Transcription
# ========================================
# Model size: tiny, base, small, medium, large
WHISPER_MODEL=small

# ========================================
# Server Configuration
# ========================================
HOST=127.0.0.1
PORT=8000

# ========================================
# Database Configuration
# ========================================
# ChromaDB persistence directory
CHROMA_PERSIST_DIR=./data/chroma_db

# ========================================
# Feature Flags (Optional)
# ========================================
# Enable debug logging
# DEBUG=false

# Enable CORS for development
# CORS_ENABLED=true
